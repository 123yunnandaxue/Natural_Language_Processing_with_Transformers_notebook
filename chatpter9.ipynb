{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP6VgC8Lxf3C"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 构建一个GitHub Issue标注程序"
      ],
      "metadata": {
        "id": "uSbela6pxr5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset_url = \"https://git.io/nlp-with-transformers\"\n",
        "df_issues = pd.read_json(dataset_url, lines=True)\n",
        "print(f\"DataFrame shape: {df_issues.shape}\")\n",
        "cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\n",
        "\n",
        "df_issues.loc[2, cols].to_frame()\n"
      ],
      "metadata": {
        "id": "pRTA0_N_xt_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    {\n",
        "        \"id\":2659267025, \"node_id\":\"MDU6TGFiZWwyNjU5MjY3MDI1\",\n",
        "        \"url\":\"https://api.github.com/repos/huggingface...\",\n",
        "        \"name\":\"DeepSpeed\",\n",
        "        \"color\":\"4D34F7\",\n",
        "        \"default\":false,\n",
        "        \"description\":\"\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "faVAB9c1xyLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"labels\"] = (df_issues[\"labels\"] .apply(lambda x: [meta[\"name\"] for meta in x]))\n",
        "df_issues[[\"labels\"]].head()\n"
      ],
      "metadata": {
        "id": "zz-a40RFxzL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"labels\"].apply(lambda x : len(x)).value_counts().to_frame().T\n"
      ],
      "metadata": {
        "id": "87qFn20Dx0PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
        "print(f\"Number of labels: {len(df_counts)}\")"
      ],
      "metadata": {
        "id": "SvdV8Q7Vx1Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"Core: Tokenization\": \"tokenization\", \"New model\": \"new model\", \"Core: Modeling\": \"model training\", \"Usage\": \"usage\", \"Core: Pipeline\": \"pipeline\", \"TensorFlow\": \"tensorflow or tf\", \"PyTorch\": \"pytorch\", \"Examples\": \"examples\", \"Documentation\": \"documentation\"}\n",
        "\n",
        "def filter_labels(x):\n",
        "\treturn [label_map[label] for label in x if label in label_map]\n",
        "\n",
        "df_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\n",
        "all_labels = list(label_map.values())\n"
      ],
      "metadata": {
        "id": "kHpUV9zKx3Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_counts = df_issues[\"labels\"].explode().value_counts()\n",
        "df_counts.to_frame().T"
      ],
      "metadata": {
        "id": "Yu9DLSdCx38P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"split\"] = \"unlabeled\"\n",
        "mask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\n",
        "df_issues.loc[mask, \"split\"] = \"labeled\"\n",
        "df_issues[\"split\"].value_counts().to_frame()\n"
      ],
      "metadata": {
        "id": "G77WFH2Jx5Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in [\"title\", \"body\", \"labels\"]:\n",
        "\tprint(f\"{column}: {df_issues[column].iloc[26][:500]}\\n\")"
      ],
      "metadata": {
        "id": "CfYPvhCcx6Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues[\"text\"] = (df_issues .apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1))\n",
        "len_before = len(df_issues)\n",
        "df_issues = df_issues.drop_duplicates(subset=\"text\")\n",
        "print(f\"Removed {(len_before-len(df_issues))/len_before:.2%} duplicates.\")\n"
      ],
      "metadata": {
        "id": "H2WdIbcjx93E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(df_issues[\"text\"].str.split().apply(len) .hist(bins=np.linspace(0, 500, 50), grid=False, edgecolor=\"C0\"))\n",
        "plt.title(\"Words per issue\")\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.ylabel(\"Number of issues\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GeakSqllyANm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([all_labels])\n",
        "mlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])"
      ],
      "metadata": {
        "id": "zNxpZUkTyE9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "def balanced_split(df, test_size=0.5):\n",
        "\tind = np.expand_dims(np.arange(len(df)), axis=1)\n",
        "\tlabels = mlb.transform(df[\"labels\"])\n",
        "\tind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, test_size)\n",
        "    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:,0]]\n"
      ],
      "metadata": {
        "id": "KtD3fAbbyHsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\n",
        "df_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\n",
        "df_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n",
        "\n",
        "np.random.seed(0)\n",
        "df_train, df_tmp = balanced_split(df_sup, test_size=0.5)\n",
        "df_valid, df_test = balanced_split(df_tmp, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "OjqWeiSqyInm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "ds = DatasetDict({ \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)), \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)), \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)), \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))})\n"
      ],
      "metadata": {
        "id": "LKM6R1iTyJnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "all_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\n",
        "indices_pool = all_indices\n",
        "labels = mlb.transform(ds[\"train\"][\"labels\"])\n",
        "train_samples = [8, 16, 32, 64, 128]\n",
        "train_slices, last_k = [], 0\n",
        "for i, k in enumerate(train_samples):\n",
        "\t# Split off samples necessary to fill the gap to the next split size\n",
        "\tindices_pool, labels, new_slice, _ = iterative_train_test_split( indices_pool, labels, (k-last_k)/len(labels))\n",
        "\tlast_k = k\n",
        "\tif i==0:\n",
        "\t\ttrain_slices.append(new_slice)\n",
        "\telse:\n",
        "\t\ttrain_slices.append(np.concatenate((train_slices[-1], new_slice)))\n",
        "# Add full dataset as last slice\n",
        "train_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\n",
        "train_slices = [np.squeeze(train_slice) for train_slice in train_slices]\n"
      ],
      "metadata": {
        "id": "8h0AmX6pyKfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Target split sizes:\")\n",
        "print(train_samples)\n",
        "print(\"Actual split sizes:\")\n",
        "print([len(x) for x in train_slices])\n"
      ],
      "metadata": {
        "id": "2VCjMylnyLdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 实现朴素贝叶斯基线"
      ],
      "metadata": {
        "id": "0HNlEe8YyQbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_labels(batch):\n",
        "\tbatch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
        "\treturn batch\n",
        "ds = ds.map(prepare_labels, batched=True)\n",
        "from collections import defaultdict\n",
        "\n",
        "macro_scores, micro_scores = defaultdict(list), defaultdict(list)\n"
      ],
      "metadata": {
        "id": "Z6hCwc9LyOaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "for train_slice in train_slices:\n",
        "\t# Get training slice and test data\n",
        "\tds_train_sample = ds[\"train\"].select(train_slice)\n",
        "\ty_train = np.array(ds_train_sample[\"label_ids\"])\n",
        "\ty_test = np.array(ds[\"test\"][\"label_ids\"])\n",
        "\t# Use a simple count vectorizer to encode our texts as token counts\n",
        "\tcount_vect = CountVectorizer()\n",
        "\tX_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n",
        "\tX_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n",
        "\t# Create and train our model!\n",
        "\tclassifier = BinaryRelevance(classifier=MultinomialNB()) classifier.fit(X_train_counts, y_train)\n",
        "\t# Generate predictions and evaluate\n",
        "\ty_pred_test = classifier.predict(X_test_counts)\n",
        "\tclf_report = classification_report( y_test, y_pred_test, target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
        "\t# Store metrics\n",
        "\tmacro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
        "\tmicro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n"
      ],
      "metadata": {
        "id": "EDu7lzntyVpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n",
        "\tfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
        "\tfor run in micro_scores.keys():\n",
        "\t\tif run == current_model:\n",
        "\t\t\tax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n",
        "\t\t\tax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n",
        "\t\telse:\n",
        "\t\t\tax0.plot(sample_sizes, micro_scores[run], label=run, linestyle=\"dashed\")\n",
        "\t\t\tax1.plot(sample_sizes, macro_scores[run], label=run, linestyle=\"dashed\")\n",
        "\tax0.set_title(\"Micro F1 scores\")\n",
        "\tax1.set_title(\"Macro F1 scores\")\n",
        "\tax0.set_ylabel(\"Test set F1 score\")\n",
        "\tax0.legend(loc=\"lower right\")\n",
        "\tfor ax in [ax0, ax1]:\n",
        "\t\tax.set_xlabel(\"Number of training samples\")\n",
        "\t\tax.set_xscale(\"log\")\n",
        "\t\tax.set_xticks(sample_sizes)\n",
        "\t\tax.set_xticklabels(sample_sizes)\n",
        "\t\tax.minorticks_off()\n",
        "\tplt.tight_layout()\n",
        "\tplt.show()\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")\n"
      ],
      "metadata": {
        "id": "Jn_dgH-3yZcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 零样本学习"
      ],
      "metadata": {
        "id": "kNS6ADAHymm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "movie_desc = \"The main characters of the movie madacascar \\ are a lion, a zebra, a giraffe, and a hippo. \"\n",
        "prompt = \"The movie is about [MASK].\"\n",
        "output = pipe(movie_desc + prompt)\n",
        "for element in output:\n",
        "\tprint(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n",
        ""
      ],
      "metadata": {
        "id": "ejYmS279ymFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\n",
        "for element in output:\n",
        "\tprint(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n",
        ""
      ],
      "metadata": {
        "id": "XzZDtppeyksf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_desc = \"In the movie transformers aliens \\ can morph into a wide range of vehicles.\"\n",
        "output = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\n",
        "for element in output:\n",
        "\tprint(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")\n"
      ],
      "metadata": {
        "id": "4pA7gJoeyuaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"zero-shot-classification\", device=0)\n",
        "sample = ds[\"train\"][0]\n",
        "print(f\"Labels: {sample['labels']}\")\n",
        "output = pipe(sample[\"text\"], all_labels, multi_label=True)\n",
        "print(output[\"sequence\"][:400])\n",
        "print(\"\\nPredictions:\")\n",
        "for label, score in zip(output[\"labels\"], output[\"scores\"]):\n",
        "\tprint(f\"{label}, {score:.2f}\")"
      ],
      "metadata": {
        "id": "F3zrBZIYyxcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_pipeline(example):\n",
        "\toutput = pipe(example[\"text\"], all_labels, multi_label=True)\n",
        "\texample[\"predicted_labels\"] = output[\"labels\"]\n",
        "\texample[\"scores\"] = output[\"scores\"]\n",
        "\treturn example\n",
        "ds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)\n"
      ],
      "metadata": {
        "id": "4zOqBQYBy1HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(example, threshold=None, topk=None):\n",
        "\tpreds = []\n",
        "\tif threshold:\n",
        "\t\tfor label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n",
        "\t\t\tif score >= threshold:\n",
        "\t\t\t\tpreds.append(label)\n",
        "\telif topk:\n",
        "\t\tfor i in range(topk):\n",
        "            preds.append(example[\"predicted_labels\"][i])\n",
        "    else:\n",
        "         raise ValueError(\"Set either `threshold` or `topk`.\")\n",
        "    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}\n"
      ],
      "metadata": {
        "id": "4nEYvTczy2W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clf_report(ds):\n",
        "\ty_true = np.array(ds[\"label_ids\"])\n",
        "\ty_pred = np.array(ds[\"pred_label_ids\"])\n",
        "\treturn classification_report( y_true, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True)\n"
      ],
      "metadata": {
        "id": "U7NYmZA6y3ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macros, micros = [], []\n",
        "topks = [1, 2, 3, 4]\n",
        "for topk in topks:\n",
        "\tds_zero_shot = ds_zero_shot.map(get_preds, batched=False, fn_kwargs={'topk': topk})\n",
        "\tclf_report = get_clf_report(ds_zero_shot)\n",
        "\tmicros.append(clf_report['micro avg']['f1-score'])\n",
        "\tmacros.append(clf_report['macro avg']['f1-score'])\n",
        "plt.plot(topks, micros, label='Micro F1')\n",
        "plt.plot(topks, macros, label='Macro F1')\n",
        "plt.xlabel(\"Top-k\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BPFMiGwwy4v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macros, micros = [], []\n",
        "thresholds = np.linspace(0.01, 1, 100)\n",
        "for threshold in thresholds:\n",
        "\tds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={\"threshold\": threshold})\n",
        "\tclf_report = get_clf_report(ds_zero_shot)\n",
        "    micros.append(clf_report[\"micro avg\"][\"f1-score\"])\n",
        "    macros.append(clf_report[\"macro avg\"][\"f1-score\"])\n",
        "plt.plot(thresholds, micros, label=\"Micro F1\")\n",
        "plt.plot(thresholds, macros, label=\"Macro F1\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8tU3FhLmy521"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\n",
        "print(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.') best_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\n",
        "print(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')\n"
      ],
      "metadata": {
        "id": "umVxde9My7FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_zero_shot = ds['test'].map(zero_shot_pipeline)\n",
        "ds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={'topk': 1})\n",
        "clf_report = get_clf_report(ds_zero_shot)\n",
        "for train_slice in train_slices:\n",
        "\tmacro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n",
        "\tmicro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")\n"
      ],
      "metadata": {
        "id": "hXRnvDX2y9vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 少样本学习"
      ],
      "metadata": {
        "id": "yd6_n_6ay_mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "set_seed(3)\n",
        "aug = naw.ContextualWordEmbsAug(model_path=\"distilbert-base-uncased\", device=\"cpu\", action=\"substitute\")\n",
        "text = \"Transformers are the most popular toys\"\n",
        "print(f\"Original text: {text}\")\n",
        "print(f\"Augmented text: {aug.augment(text)}\")"
      ],
      "metadata": {
        "id": "9A_KzC4jzBat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_text(batch, transformations_per_example=1):\n",
        "\ttext_aug, label_ids = [], []\n",
        "\tfor text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n",
        "\t\ttext_aug += [text]\n",
        "\t\tlabel_ids += [labels]\n",
        "\t\tfor _ in range(transformations_per_example):\n",
        "\t\t\ttext_aug += [aug.augment(text)]\n",
        "\t\t\tlabel_ids += [labels]\n",
        "    return {\"text\": text_aug, \"label_ids\": label_ids}\n",
        "ds_train_sample = ds_train_sample.map(augment_text, batched=True, remove_columns=ds_train_sample.column_names).shuffle(seed=42)\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")\n"
      ],
      "metadata": {
        "id": "mFhx45PizEje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "model_ckpt = \"miguelvictor/python-gpt2-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "\t# Extract the token embeddings\n",
        "\ttoken_embeddings = model_output[0]\n",
        "\t# Compute the attention mask\n",
        "\tinput_mask_expanded = (attention_mask .unsqueeze(-1) .expand(token_embeddings.size()) .float())\n",
        "\t# Sum the embeddings, but ignore masked tokens\n",
        "\tsum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "\tsum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\t# Return the average as a single vector\n",
        "\treturn sum_embeddings / sum_mask\n",
        "def embed_text(examples):\n",
        "\tinputs = tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\twith torch.no_grad():\n",
        "\t\tmodel_output = model(**inputs)\n",
        "\tpooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n",
        "\treturn {\"embedding\": pooled_embeds.cpu().numpy()}\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "embs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\n",
        "embs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\n",
        "embs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)\n",
        "embs_train.add_faiss_index(\"embedding\")\n"
      ],
      "metadata": {
        "id": "cHisGQzUzIy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i, k = 0, 3\n",
        "# Select the first query and 3 nearest neighbors\n",
        "rn, nl = \"\\r\\n\\r\\n\", \"\\n\"\n",
        "# Used to remove newlines in text for compact display\n",
        "query = np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\n",
        "scores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\n",
        "print(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\n",
        "print(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn, nl)} [...]\\n\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Retrieved documents:\")\n",
        "for score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n",
        "\tprint(\"=\"*50)\n",
        "\tprint(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n",
        "\tprint(f\"SCORE: {score:.2f}\")\n",
        "\tprint(f\"LABELS: {label}\")"
      ],
      "metadata": {
        "id": "3r-lK_VFzMr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_preds(sample, m):\n",
        "\treturn (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)\n",
        "def find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n",
        "\tmax_k = min(len(ds_train), max_k)\n",
        "\tperf_micro = np.zeros((max_k, max_k))\n",
        "\tperf_macro = np.zeros((max_k, max_k))\n",
        "\tfor k in range(1, max_k):\n",
        "\t\tfor m in range(1, k + 1):\n",
        "        \t_, samples = ds_train.get_nearest_examples_batch(\"embedding\", valid_queries, k=k)\n",
        "        \ty_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
        "        \tclf_report = classification_report(valid_labels, y_pred\n",
        "target_names=mlb.classes_, zero_division=0, output_dict=True)\n",
        "\t\t\tperf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n",
        "\t\t\tperf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n",
        "\treturn perf_micro, perf_macro\n"
      ],
      "metadata": {
        "id": "zATCcrlOzRjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels = np.array(embs_valid[\"label_ids\"])\n",
        "valid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\n",
        "perf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\n",
        "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
        "ax0.imshow(perf_micro)\n",
        "ax1.imshow(perf_macro)\n",
        "ax0.set_title(\"micro scores\")\n",
        "ax0.set_ylabel(\"k\")\n",
        "ax1.set_title(\"macro scores\")\n",
        "for ax in [ax0, ax1]:\n",
        "\tax.set_xlim([0.5, 17 - 0.5])\n",
        "\tax.set_ylim([17 - 0.5, 0.5])\n",
        "\tax.set_xlabel(\"m\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XJnI-WoizRuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n",
        "print(f\"Best k: {k}, best m: {m}\")\n",
        "embs_train.drop_index(\"embedding\")\n",
        "test_labels = np.array(embs_test[\"label_ids\"])\n",
        "test_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n",
        "for train_slice in train_slices:\n",
        "# Create a Faiss index from training slice\n",
        "\tembs_train_tmp = embs_train.select(train_slice)\n",
        "\tembs_train_tmp.add_faiss_index(\"embedding\")\n",
        "    # Get best k, m values with validation set\n",
        "    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n",
        "    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n",
        "    # Get predictions on test set\n",
        "    _, samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\", test_queries, k=int(k))\n",
        "    y_pred = np.array([get_sample_preds(s, m) for s in samples])\n",
        "    # Evaluate predictions\n",
        "    clf_report = classification_report(test_labels, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True,)\n",
        "    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n",
        "    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"]\n",
        "\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VdGzJqEgzUak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (AutoTokenizer, AutoConfig, AutoModelForSequenceClassification)\n",
        "\n",
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "def tokenize(batch):\n",
        "\treturn tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
        "ds_enc = ds.map(tokenize, batched=True)\n",
        "ds_enc = ds_enc.remove_columns(['labels', 'text'])\n",
        "ds_enc.set_format(\"torch\")\n",
        "ds_enc = ds_enc.map(lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)}, remove_columns=[\"label_ids\"])\n",
        "ds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")\n"
      ],
      "metadata": {
        "id": "UVEfLGHCzYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "training_args_fine_tune = TrainingArguments( output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5, lr_scheduler_type='constant', per_device_train_batch_size=4, per_device_eval_batch_size=32, weight_decay=0.0, evaluation_strategy=\"epoch\", save_strategy=\"epoch\",logging_strategy=\"epoch\", load_best_model_at_end=True, metric_for_best_model='micro f1', save_total_limit=1, log_level='error')\n",
        "from scipy.special import expit as sigmoid\n",
        "def compute_metrics(pred):\n",
        "\ty_true = pred.label_ids\n",
        "    y_pred = sigmoid(pred.predictions)\n",
        "    y_pred = (y_pred>0.5).astype(float)\n",
        "    clf_dict = classification_report(y_true, y_pred, target_names=all_labels, zero_division=0, output_dict=True)\n",
        "    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"], \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}\n"
      ],
      "metadata": {
        "id": "-PoWP7rMzad1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "config.num_labels = len(all_labels)\n",
        "config.problem_type = \"multi_label_classification\"\n",
        "for train_slice in train_slices:\n",
        "\tmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
        "\ttrainer = Trainer( model=model, tokenizer=tokenizer, args=training_args_fine_tune, compute_metrics=compute_metrics, train_dataset=ds_enc[\"train\"].select(train_slice), eval_dataset=ds_enc[\"valid\"],)\n",
        "\ttrainer.train()\n",
        "\tpred = trainer.predict(ds_enc[\"test\"])\n",
        "\tmetrics = compute_metrics(pred)\n",
        "\tmacro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n",
        "\tmicro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])\n",
        "\n",
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")\n"
      ],
      "metadata": {
        "id": "tjYr_Q_4zgJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\\ Translate English to French: thanks => \"\"\"\n",
        "def tokenize(batch):\n",
        "\treturn tokenizer(batch[\"text\"], truncation=True, max_length=128, return_special_tokens_mask=True)\n",
        "\tds_mlm = ds.map(tokenize, batched=True)\n",
        "\tds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])\n",
        "from transformers import DataCollatorForLanguageModeling, set_seed\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "set_seed(3)\n",
        "data_collator.return_tensors = \"np\"\n",
        "inputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\n",
        "outputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\n",
        "\n",
        "pd.DataFrame({ \"Original tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"] [0]), \"Masked tokens\": tokenizer.convert_ids_to_tokens(outputs[\"input_ids\"][0]), \"Original input_ids\": original_input_ids, \"Masked input_ids\": masked_input_ids, \"Labels\": outputs[\"labels\"][0]}).T\n",
        "data_collator.return_tensors = \"pt\"\n",
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "training_args = TrainingArguments( output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32, logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\", num_train_epochs=16,log_level=\"error\", report_to=\"none\")\n",
        "\n",
        "trainer = Trainer( model=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\"), tokenizer=tokenizer, args=training_args, data_collator=data_collator, train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"])\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "wXDkEsaQzjOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_log = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"] .plot(label=\"Validation\"))\n",
        "\n",
        "df_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zl6bZEiZzwjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = f'{model_ckpt}-issues-128'\n",
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "config.num_labels = len(all_labels)\n",
        "config.problem_type = \"multi_label_classification\"\n",
        "for train_slice in train_slices:\n",
        "\tmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
        "\ttrainer = Trainer( model=model, tokenizer=tokenizer, args=training_args_fine_tune, compute_metrics=compute_metrics\n",
        "  train_dataset=ds_enc[\"train\"].select(train_slice), eval_dataset=ds_enc[\"valid\"], )\n",
        "\ttrainer.train()\n",
        "\tpred = trainer.predict(ds_enc['test'])\n",
        "\tmetrics = compute_metrics(pred)\n",
        "\t # DA refers to domain adaptation\n",
        "\tmacro_scores['Fine-tune (DA)'].append(metrics['macro f1'])\n",
        "\tmicro_scores['Fine-tune (DA)'].append(metrics['micro f1'])\n"
      ],
      "metadata": {
        "id": "DiTwDR2Uzx9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")\n"
      ],
      "metadata": {
        "id": "sZihDZW_z6Iz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}