{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PajkCKic0yp_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 从头训练 Transformers 模型"
      ],
      "metadata": {
        "id": "EFcozMTa08AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
        "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "def model_size(model):\n",
        "\treturn sum(t.numel() for t in model.parameters())\n",
        "\n",
        "print(f\"GPT size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
        "\n",
        "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")\n"
      ],
      "metadata": {
        "id": "QHnH4o3c08iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
        "\tout = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
        "\treturn \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
        "\n",
        "prompt = \"\\nWhen they came back\"\n",
        "print(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
        "print(\"\") print(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
      ],
      "metadata": {
        "id": "Q1fuF6dh1CwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DownloadConfig\n",
        "download_config = DownloadConfig(delete_extracted=True)\n",
        "\n",
        "dataset = load_dataset(\"./codeparrot\", split=\"train\", download_config=download_config)\n"
      ],
      "metadata": {
        "id": "QZQBkLKo1Jzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
        "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
        "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
        "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
        "# Process.memory_info is expressed in bytes, so we convert to MB\n",
        "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")\n",
        "streamed_dataset = load_dataset('./codeparrot', split=\"train\", streaming=True)\n",
        "iterator = iter(streamed_dataset)\n",
        "print(dataset[0] == next(iterator))"
      ],
      "metadata": {
        "id": "e2QQDLLd1Puc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset = load_dataset('transformersbook/codeparrot', split=\"train\", streaming=True)\n"
      ],
      "metadata": {
        "id": "fZjot_WF1ULl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 构建一个标记化器 （tokenizer)"
      ],
      "metadata": {
        "id": "nLBTgioa1Xw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "def tok_list(tokenizer, string):\n",
        "\tinput_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
        "\treturn [tokenizer.decode(tok) for tok in input_ids]\n",
        "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
        "print(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')"
      ],
      "metadata": {
        "id": "419dNI8z1gPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "python_code = r\"\"\"\n",
        "def say_hello():\n",
        "\tprint(\"Hello, World!\")\n",
        "# Print it say_hello() \"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(tokenizer(python_code).tokens())\n",
        "print(tokenizer.backend_tokenizer.normalizer)\n",
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
      ],
      "metadata": {
        "id": "7cKsZbGh1jXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, e = u\"a\", u\"€\"\n",
        "byte = ord(a.encode(\"utf-8\"))\n",
        "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
        "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
        "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')"
      ],
      "metadata": {
        "id": "z583ROqR1p2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.gpt2.tokenization_gpt2\n",
        "import bytes_to_unicode\n",
        "byte_to_unicode_map = bytes_to_unicode()\n",
        "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
        "base_vocab = list(unicode_to_byte_map.keys())\n",
        "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
        "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')\n",
        "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))\n",
        "print(f\"Size of the vocabulary: {len(tokenizer)}\")\n",
        "print(tokenizer(python_code).tokens())\n",
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]); ['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
        "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:12]]);"
      ],
      "metadata": {
        "id": "6B9le1L21uHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "length = 10000\n",
        "dataset_name = 'transformersbook/codeparrot-train'\n",
        "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
        "iter_dataset = iter(dataset)\n",
        "def batch_iterator(batch_size=10):\n",
        "\tfor _ in tqdm(range(0, length, batch_size)):\n",
        "\t\tyield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
        "\n",
        "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=12500, initial_alphabet=base_vocab)\n"
      ],
      "metadata": {
        "id": "d-JxvrFn17ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
        "\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);\n",
        "print([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);\n",
        "print(new_tokenizer(python_code).tokens())"
      ],
      "metadata": {
        "id": "PEhIsDq218vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keyword print(f'There are in total {len(keyword.kwlist)} Python keywords.')\n",
        "for keyw in keyword.kwlist:\n",
        "\tif keyw not in new_tokenizer.vocab:\n",
        "\t\tprint(f'No, keyword `{keyw}` is not in the vocabulary')\n"
      ],
      "metadata": {
        "id": "PoJytPiO2EmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = 200000\n",
        "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=32768, initial_alphabet=base_vocab)\n"
      ],
      "metadata": {
        "id": "8__PONQj2Hc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1], reverse=False)\n",
        "print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);\n",
        "print(new_tokenizer_larger(python_code).tokens())"
      ],
      "metadata": {
        "id": "oMOWTSUk2Itq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for keyw in keyword.kwlist:\n",
        "\tif keyw not in new_tokenizer_larger.vocab:\n",
        "    \tprint(f'No, keyword `{keyw}` is not in the vocabulary')\n",
        "\n",
        " No, keyword `nonlocal` is not in the vocabulary\n"
      ],
      "metadata": {
        "id": "5TNp4Yvg2Lfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 从头开始训练一个模型"
      ],
      "metadata": {
        "id": "kQ_5shT92Q-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
        "model_small = AutoModelForCausalLM.from_config(config_small)\n",
        "print(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')\n",
        "input_characters = number_of_sequences * sequence_length * characters_per_token\n"
      ],
      "metadata": {
        "id": "9KWX43u32R27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples, total_characters, total_tokens = 500, 0, 0\n",
        "dataset = load_dataset('transformersbook/codeparrot-train', split='train', streaming=True)\n",
        "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
        "\ttotal_characters += len(example['content'])\n",
        "\ttotal_tokens += len(tokenizer(example['content']).tokens())\n",
        "characters_per_token = total_characters / total_tokens print(characters_per_token) 3.6233025034779565\n"
      ],
      "metadata": {
        "id": "mxaGZj152cso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "  def __init__(self,tokenizer,dataset,seq_length=1024,num_of_sequences=1024,chars_per_token=3.6):\n",
        "    self.tokenizer=tokenizer\n",
        "    self.concat_token_id=tokenizer.eos_token_id\n",
        "    self.dataset=dataset\n",
        "    self.seq_length=seq_length\n",
        "    self.input_characters=seq_length*char_per_token*num_of_sequences\n",
        "  def __iter__(self):\n",
        "    iterator=iter(self.dataset)\n",
        "    more_examples=True\n",
        "    while more_examples:\n",
        "      buffer,buffer_len=[],0\n",
        "      while True:\n",
        "        if buffer_len>=self.input_characters:\n",
        "          m=f\"Buffer full:{buffer_len}>={self.input_characters:.0f}\"\n",
        "          print(m)\n",
        "          break\n",
        "        try:\n",
        "          m=f'Fill buffer:{buffer_len}<{self.input_characters:.0f}'\n",
        "          print(m)\n",
        "          buffer.append(next(iterator)['content'])\n",
        "          buffer_len+=len(buffer[-1])\n",
        "        except StopIteration:\n",
        "          iterator=iter(self.dataset)\n",
        "      all_token_ids=[]\n",
        "      tokenized_inputs=self.tokenizer(buffer,trunction=False)\n",
        "      for tokenized_input in tokenized_inputs['input_ids']:\n",
        "        all_token_ids.extend(tokenized_input+[self.concat_token_id])\n",
        "      for i in range(0,len(all_token_ids),self.seq_length):\n",
        "        input_ids=all_token_ids[i:i+self.seq_length]\n",
        "        if len(input_ids)==self.seq_length:\n",
        "          yield torch.tensor(input_ids)"
      ],
      "metadata": {
        "id": "UjZPcjOV2eQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
        "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset, num_of_sequences=10)\n",
        "dataset_iterator = iter(constant_length_dataset)\n",
        "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
        "print(f\"Lengths of the sequences: {lengths}\")"
      ],
      "metadata": {
        "id": "nuZG4soN53Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from accelerate import Accelerator\n",
        "device='cpu'\n",
        "accelerator=Accelerator()\n",
        "model=torch.nn.Transformer().to(device)\n",
        "model=torch.nn.Transformer()\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "dataset=load_dataset('my_dataset')\n",
        "data=torch.utils.data.DataLorader(dataset,shuffle=True)\n",
        "model,optimizer,data=accelerator.prepare(model,optimizer,data)\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "  for source,targets in data:\n",
        "    source=source.to(device)\n",
        "    targets=targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output=model(source)\n",
        "    loss=F.cross_entropy(output,targets)\n",
        "    loss.backward()\n",
        "    accelerator.backward(loss)\n",
        "    optimizer.step()\n",
        ""
      ],
      "metadata": {
        "id": "FAEpV0Op55T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "# Commented parameters correspond to the small model\n",
        "config = {\"train_batch_size\": 2, # 12\n",
        "        \"valid_batch_size\": 2, # 12\n",
        "        \"weight_decay\": 0.1, \"shuffle_buffer\": 1000, \"learning_rate\": 2e-4, # 5e-4\n",
        "        \"lr_scheduler_type\": \"cosine\", \"num_warmup_steps\": 750, # 2000\n",
        "        \"gradient_accumulation_steps\": 16, # 1 \"max_train_steps\": 50000, # 150000\n",
        "        \"max_eval_steps\": -1, \"seq_length\": 1024, \"seed\": 1, \"save_checkpoint_steps\": 50000} # 15000\n",
        "args = Namespace(**config)\n"
      ],
      "metadata": {
        "id": "w6cWHkYGD4fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import wandb\n",
        "def setup_logging(project_name):\n",
        "  logger=logging.getLogger(__name__)\n",
        "  logging.basicConfig(\n",
        "      format='%(asctime)s-%(levelname)s-%(name)s-%(message)s',\n",
        "      datefmt='%m/%d/%Y %H:%M:%S',level=logging.INFO,handlers=[\n",
        "          logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\")\n",
        "          logging.StreamHandler()\n",
        "      ]\n",
        "  )\n",
        "  if accelerator.is_main_process:\n",
        "    wandb.init(project=project_name,config=args)\n",
        "    run_name=wandb.run.name\n",
        "    tb_writer=SummaryWriter()\n",
        "    tb_wirter.add_hparams(vars(args),{'0':0})\n",
        "    logger.setLevel(logging.INFO)\n",
        "    datasets.utils.logging.set_verbosity_debug()\n",
        "    transformers.utils.logging.set_verbosity_info()\n",
        "  else:\n",
        "    tb_writer=None\n",
        "    run_name=''\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    datasets.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "  return logger,tb_writer,run_name\n"
      ],
      "metadata": {
        "id": "O7yPeFDjD6-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics(step, metrics):\n",
        "\tlogger.info(f\"Step {step}: {metrics}\"\n",
        "\tif accelerator.is_main_process:\n",
        "\t\twandb.log(metrics) [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]\n"
      ],
      "metadata": {
        "id": "0vV46a22TJAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZqNF5FRTJvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}